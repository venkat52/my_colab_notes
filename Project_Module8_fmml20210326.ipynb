{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkat52/my_colab_notes/blob/main/Project_Module8_fmml20210326.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7GMI8r9cdMQN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import typing\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianMulticlassModel:\n",
        "    \"\"\"\n",
        "    A multi-class bayesian classfier from encoded text tokens\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, num_tokens) -> None:\n",
        "        self.counts = np.zeros(shape=(num_classes, num_tokens))\n",
        "    def fit(self, x_train: typing.Iterable[np.ndarray], y_train: typing.Iterable[int]):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            \n",
        "            self.counts[y] += x\n",
        "    def predict(self, counts_vector):\n",
        "        class_frequencies = np.sum(self.counts, axis=1)\n",
        "        word_frequencies = np.sum(self.counts, axis=0)\n",
        "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
        "        likelihood = self.counts / np.expand_dims(\n",
        "            class_frequencies, axis=1\n",
        "        )  # p(word|label)\n",
        "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
        "        likelihood = np.multiply(likelihood, counts_vector)\n",
        "        prior = np.expand_dims(prior, axis=1)\n",
        "        posterior_marginal = prior * likelihood / evidence + 0.00001\n",
        "        posterior_joint = np.sum(np.log(posterior_marginal), axis=1)\n",
        "        return np.flip(np.argsort(posterior_joint))"
      ],
      "metadata": {
        "id": "bemLVFhwdnxq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "IjorLRQhdn0e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfreader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kTnVnQceOxv",
        "outputId": "caaad3af-560e-4f2c-e2c8-fd16d99f95b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfreader\n",
            "  Downloading pdfreader-0.1.11.tar.gz (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 4.0 MB/s \n",
            "\u001b[?25hCollecting bitarray>=1.1.0\n",
            "  Downloading bitarray-2.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 42.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from pdfreader) (7.1.2)\n",
            "Collecting pycryptodome>=3.9.9\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from pdfreader) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.1->pdfreader) (1.15.0)\n",
            "Building wheels for collected packages: pdfreader\n",
            "  Building wheel for pdfreader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfreader: filename=pdfreader-0.1.11-py3-none-any.whl size=134240 sha256=3bb68eb4dbb368e39e8aab0d5dfe7f5d931c686f231bbd5b3e605200f6be23ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/64/62/480f184e5d801d391bae6ca86ec1af0b9e8737d41fd5cbb274\n",
            "Successfully built pdfreader\n",
            "Installing collected packages: pycryptodome, bitarray, pdfreader\n",
            "Successfully installed bitarray-2.5.1 pdfreader-0.1.11 pycryptodome-3.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_LENGTH_THRESHOLD = 2\n",
        "WORD_COUNT_THRESHOLD = 1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pdfreader import PDFDocument, SimplePDFViewer, document"
      ],
      "metadata": {
        "id": "oDFq_85udn2m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str):\n",
        "    \"\"\"\n",
        "    Given text it removes all the non-character words, small words,\n",
        "    converts everything to small letters, tokenizes and returns as a list.\n",
        "    :param text: The text to be cleaned\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-z]\", \" \", text)\n",
        "    data = text.split()\n",
        "    data = list(filter(lambda x: len(x) >= WORD_LENGTH_THRESHOLD, data))\n",
        "    return data"
      ],
      "metadata": {
        "id": "P2py6NvUdn5M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_pdf(filename: str):\n",
        "    \"\"\"\n",
        "    Read text from a PDF file.\n",
        "    Clean the text, tokenize it, and return as a list of tokens.\n",
        "    :param :\n",
        "    \"\"\"\n",
        "    fd = open(filename, \"rb\")\n",
        "    document = PDFDocument(fd)\n",
        "    viewer = SimplePDFViewer(fd)\n",
        "    output_strings = []\n",
        "    for i in range(len(list(document.pages()))):\n",
        "        viewer.navigate(1)\n",
        "        viewer.render()\n",
        "        output_strings.extend(viewer.canvas.strings)\n",
        "    file_contents = \" \".join(output_strings)\n",
        "    return clean_text(file_contents)"
      ],
      "metadata": {
        "id": "kibEsWF8dn73"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_resume_df():\n",
        "    resume_df = pd.read_csv(\"/content/resume-dataset.csv\")\n",
        "    resume_df[\"Keywords\"] = resume_df[\"Resume\"].apply(clean_text)\n",
        "    return resume_df[\"Keywords\"].values, resume_df[\"Category\"].values"
      ],
      "metadata": {
        "id": "gnMGLidbdn_R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelEncoder:\n",
        "    \"\"\"\n",
        "    Label encode a series of labels\n",
        "    \"\"\"\n",
        "    def __init__(self, data) -> None:\n",
        "        self.__training_data = data\n",
        "        self.index_to_token = list(set(data))\n",
        "        self.token_to_index = {\n",
        "            token: index for index, token in enumerate(self.index_to_token)\n",
        "        }\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_index)\n",
        "    @property\n",
        "    def encoded_data(self):\n",
        "        return np.array([self.token_to_index[token] for token in self.__training_data])\n",
        "    def encode(self, data):\n",
        "        return np.array([self.token_to_index[token] for token in data])\n",
        "    def decode(self, data):\n",
        "        if isinstance(data, int) or isinstance(data, np.int64):\n",
        "            return self.index_to_token[data]\n",
        "        else:\n",
        "            return np.array([self.index_to_token[index] for index in data])"
      ],
      "metadata": {
        "id": "EgDnWmRRecxt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import stat\n",
        "import typing\n",
        "class BagOfWords:\n",
        "    \"\"\"\n",
        "    A type of encoder, makes\n",
        "    \"\"\"\n",
        "    def __init__(self, data: typing.Iterable) -> None:\n",
        "        \"\"\"\n",
        "        Generate the bag of words\n",
        "        :param data: an array of words, or an iterable containing arrays of words\n",
        "        \"\"\"\n",
        "        data = np.array(self.__linearize_array(data))\n",
        "        self.index_to_words = np.unique(data)\n",
        "        self.words_to_index = {w: i for i, w in enumerate(self.index_to_words)}\n",
        "    @classmethod\n",
        "    def __linearize_array(cls, text):\n",
        "        x = []\n",
        "        for item in text:\n",
        "            if isinstance(item, str):\n",
        "                x.append(item)\n",
        "            else:\n",
        "                x.extend(cls.__linearize_array(item))\n",
        "        return x\n",
        "    def __call__(self, text: typing.Iterable[str]) -> np.array:\n",
        "        return self.get_counts(text)\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.index_to_words)\n",
        "    def encode_data(\n",
        "        self: \"BagOfWords\",\n",
        "        text: typing.Union[typing.Iterable[str], typing.Iterable[typing.Iterable[str]]],\n",
        "    ) -> np.array:\n",
        "        \"\"\"\n",
        "        Compute the encodings of words in a new input tokenized string\n",
        "        \"\"\"\n",
        "        x = []\n",
        "        for item in text:\n",
        "            if isinstance(item, str):\n",
        "                if item in self.words_to_index:\n",
        "                    x.append(self.words_to_index[item])\n",
        "            else:\n",
        "                x.append(self.encode_data(item))\n",
        "        return x\n",
        "    def decode_data(self: \"BagOfWords\", encoded_text: typing.Iterable[int]):\n",
        "        if isinstance(encoded_text, int) or isinstance(encoded_text, np.int64):\n",
        "            return self.index_to_words[encoded_text]\n",
        "        else:\n",
        "            return list(map(self.decode_data, encoded_text))\n",
        "    def get_counts(\n",
        "        self: \"BagOfWords\",\n",
        "        text: typing.Union[typing.Iterable[str], typing.Iterable[typing.Iterable[str]]],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Computes the counts of words in a new input tokenized string\n",
        "        \"\"\"\n",
        "        if len(text) == 0 or isinstance(text[0], str):\n",
        "            x = np.zeros(shape=len(self))\n",
        "            for word in text:\n",
        "                if word in self.words_to_index:\n",
        "                    x[self.words_to_index[word]] += 1\n",
        "            return x\n",
        "        else:\n",
        "            return np.stack([self.get_counts(item) for item in text], axis=0)"
      ],
      "metadata": {
        "id": "bu-8z6o5ec0K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianModelExplainer(BayesianMulticlassModel):\n",
        "    \"\"\"\n",
        "    Explainer of the decision made by the base model\n",
        "    \"\"\"\n",
        "    def __init__(self, label_encoder: LabelEncoder, bag_of_words: BagOfWords) -> None:\n",
        "        super().__init__(len(label_encoder), len(bag_of_words))\n",
        "        self.bag_of_words = bag_of_words\n",
        "        self.label_encoder = label_encoder\n",
        "    def explain(self, text=None, label_filter=None):\n",
        "        \"\"\"\n",
        "        Visualize what are the prior probabilities of classes and which words\n",
        "        add the the likelihood of each class.\n",
        "        \"\"\"\n",
        "        class_frequencies = np.sum(self.counts, axis=1)\n",
        "        word_frequencies = np.sum(self.counts, axis=0)\n",
        "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
        "        likelihood = self.counts / np.expand_dims(\n",
        "            class_frequencies, axis=1\n",
        "        )  # p(word|label)\n",
        "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
        "        if text is not None:\n",
        "            counts_vector = self.bag_of_words.get_counts(text)\n",
        "            likelihood = np.multiply(likelihood, counts_vector)\n",
        "        prior_ordering = np.flip(np.argsort(prior))\n",
        "        for item in prior_ordering:\n",
        "            likelihood = likelihood / (evidence + 0.00001)\n",
        "            label = self.label_encoder.decode(item)\n",
        "            word_ids = np.flip(np.argsort(likelihood[item]))\n",
        "            word_ids = word_ids[:10]\n",
        "            if label_filter is None or label in label_filter:\n",
        "                print(f\"{label}: {' '.join(self.bag_of_words.decode_data(word_ids))}\")"
      ],
      "metadata": {
        "id": "TV73OIvJecu1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEfIABmbdMQS",
        "outputId": "1db7a636-1040-4571-e415-eee294f74a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Science\n",
            "Automation Testing\n",
            "DotNet Developer\n",
            "Network Security Engineer\n",
            "Blockchain\n",
            "\n",
            "ANALYSIS OF TRAINED PRIOR\n",
            "-------------------------\n",
            "Operations Manager: ges korea shipments hpm qatar monitored prepaid marshalling honeywell fat\n",
            "DevOps Engineer: workload allocate northbound iseries technologists knoxed birla annet wwf solid\n",
            "PMO: te publish seat multiclient exploring synthesize prudential budgeted wfm aspiration\n",
            "Hadoop: tws rf cca sk ranges normalized hiveql increasing pathways srm\n",
            "ETL Developer: niche tconverttype corresponding cfoc domains tcacheout tcachein fx fr tsortrow\n",
            "Business Analyst: infonet relates goregoan installations configurations concerned comprehensive plastics arriving artifacts\n",
            "Java Developer: oot hit sierra effect cosmoss adavance oldest hudson manoharbhai validations\n",
            "Web Designing: fruitbuddy smartbadge pamphlet epi videos movie understood vi innovesta totalcitee\n",
            "Data Science: rathore kali deeksha reasearch segmented thesis visvesvaraya encoding pgdm pgp\n",
            "Testing: dhanraj vpi murlidhar dongare gobind reworking aggregate tap kopargaon mccia\n",
            "Database: centralize flashback compressions seconds failback sensitive fastest fdw next shifts\n",
            "Mechanical Engineer: mech ites submitting substantial conformity instiute sugarcane lifts alternatives nc\n",
            "Automation Testing: lo intouch reminders nippon neighborhood citect texas citectscada unity wonderware\n",
            "Network Security Engineer: renewals scriptingeducation careful solarwinds hld malicious dm ccnp ho sa\n",
            "SAP Developer: adaptable bottlenecks springer unicode currency tracks folder edi prince sinochem\n",
            "Blockchain: condition passengers corda fit consensuseducation osi orowealth oro opencart shree\n",
            "Python Developer: pil poster want pursuing memoization proficiencies pws pymysql esd liabray\n",
            "DotNet Developer: consuming today switzerland again cmus menswear cover stadio backed tourists\n",
            "Arts: entermediate acid storming performers entrepreneurs fellowships commissions wallace artists artist\n",
            "Electrical Engineering: erection uptime familiar metering microprocessor microsft faso shunt modulating mono\n",
            "Civil Engineer: stc airport continual dh requisition guinness nagar rcc devolopment devolopers\n",
            "Sales: bookmyflat mahal amity plots tournament grip compatible retaing taxing stretchable\n",
            "Health and fitness: asking projections homescience ansalon obesity holiday cashing whose plaza reflexology\n",
            "HR: pa preliminary cochin candidate psychotherapy celebration esic queen calicut spsseducation\n",
            "Advocate: somaiya bikaner bilingual bio aibe savvy hear journalist adalat criminology\n",
            "\n",
            "ANALYSIS OF TRAINED EVIDENCE\n",
            "----------------------------\n",
            "Operations Manager: and in for ja with of le india ma assistant\n",
            "DevOps Engineer: ai buy tion ear amazon sensor labs watch hyderabad swing\n",
            "PMO: linguistic tagging statistical delhi inter ad metrics optimization pan certification\n",
            "Hadoop: de schedulers red commands distributed applied watch star vice shopping\n",
            "ETL Developer: award together cbse applied star error long organized written here\n",
            "Business Analyst: ios sciences community passing book computers block object methods search\n",
            "Java Developer: december facebook swing subjects computers courses stack object methods oriented\n",
            "Web Designing: videos mini op shopping search ideas competition app modern structure\n",
            "Data Science: notebooks linkedin tf teaching animeshsinha google segmentation ai participants subjects\n",
            "Testing: singh mini competitive rd op cyber list current certification assembly\n",
            "Database: sync amazon gr schedulers ma statistics commands applied statistical watch\n",
            "Mechanical Engineer: coordinator contest piping supervision round rd among competition structure chain\n",
            "Automation Testing: enable bulk piping sensor star indian ap black object oct\n",
            "Network Security Engineer: sa ios stack ap filtering ideas inter content routing pan\n",
            "SAP Developer: passing round object ideas show inter oriented certification app member\n",
            "Blockchain: endpoints chess une intern react cbse block stack shopping food\n",
            "Python Developer: rank competitions round organized github filtering coding inter methods oriented\n",
            "DotNet Developer: community man book courses shopping associated list coding show content\n",
            "Arts: camp passing award art together indian total term organized ideas\n",
            "Electrical Engineering: sat ac prof statistics book courses indian associated hyderabad list\n",
            "Civil Engineer: near om piping supervision man natural round indian al list\n",
            "Sales: visualizing sciences computers entry show written here assistant best set\n",
            "Health and fitness: participants community statistical club food organized written certification app st\n",
            "HR: error hyderabad inter here assistant course july master processing school\n",
            "Advocate: sole researcher singh supervision natural indian search hyderabad member july\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    x_train, y_train = parse_resume_df()\n",
        "    bag_of_words = BagOfWords(x_train)\n",
        "    label_encoder = LabelEncoder(y_train)\n",
        "    x_train = bag_of_words.get_counts(x_train)\n",
        "    y_train = label_encoder.encode(y_train)\n",
        "    model = BayesianMulticlassModel(len(label_encoder), len(bag_of_words))\n",
        "    model.fit(x_train=x_train, y_train=y_train)\n",
        "    x_test_input = parse_pdf(\"/content/computers_2.pdf\")\n",
        "    x_test = bag_of_words.get_counts(x_test_input)\n",
        "    result = model.predict(x_train[0])\n",
        "    result = label_encoder.decode(result)\n",
        "    for job in result[:5]:\n",
        "        print(job)\n",
        "    explainable_model = BayesianModelExplainer(label_encoder, bag_of_words)\n",
        "    explainable_model.fit(x_train=x_train, y_train=y_train)\n",
        "    print(\n",
        "        \"\"\"\n",
        "ANALYSIS OF TRAINED PRIOR\n",
        "-------------------------\"\"\"\n",
        "    )\n",
        "    explainable_model.explain()\n",
        "    print(\n",
        "        \"\"\"\n",
        "ANALYSIS OF TRAINED EVIDENCE\n",
        "----------------------------\"\"\"\n",
        "    )\n",
        "    explainable_model.explain(x_test_input)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": false,
      "toc_window_display": false
    },
    "colab": {
      "name": "Project_Module8_fmml20210326.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}